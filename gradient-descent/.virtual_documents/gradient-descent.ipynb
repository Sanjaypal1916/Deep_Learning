import numpy as np 
import pandas as pd 
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split


df  = pd.read_csv("insurance_data.csv")
df.head()
X_raw  = df.drop("bought_insurance", axis=1 )
y_raw = df["bought_insurance"]



X_raw.head()


X = X_raw
X["age"] = X_raw["age"] /100


y = y_raw


X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=0, stratify=y)


X_train.shape


y_train.value_counts()


model = keras.Sequential([
    keras.layers.Dense(1,input_shape=(2,), activation="sigmoid", kernel_initializer="ones", bias_initializer= "zeros")
])

model.compile(optimizer="adam",
             loss = "binary_crossentropy",
             metrics = ["accuracy"])
model.fit(X_train, y_train, epochs=1000)


model.evaluate(X_train, y_train)
    


model.predict(X_test)


X_test


coef, intercept = model.get_weights()
coef, intercept





def sigmoid(x):
    import math
    sigm = 1 / (1 + math.exp(-x))
    return sigm

sigmoid(18)


def predict(age , affordibility ):
    weighted_sum = coef[0] * age + coef[1] * affordibility + intercept
    return sigmoid(weighted_sum)
predict(0.58, 1)


x = np.array([[0.58, 1]])
model_prediction = model.predict(X_test)
model_prediction





def sigmoid_numpy(X):
   return 1/(1+np.exp(-X))



def log_loss(y_true, y_predicted):
    epsilon = 1e-15
    y_predicted_new = [max(i,epsilon) for i in y_predicted]
    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
    y_predicted_new = np.array(y_predicted_new)
    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))





def gradient_descent(age, affordibility, epochs, y_true, loss_threshold):
    w1 = w2 = 1
    bi = 0 
    rate = 0.5
    n = len(age)

    for i in range(epochs):

            
        # initialing the chain 
        wieghted_sum = (age * w1 )+ (affordibility * w2) + bi
        y_pred = sigmoid_numpy(wieghted_sum)
        loss = log_loss(y_true, y_pred)

        #finding the derivatives
        changeinw1 = (1/n) * np.dot(np.transpose(age), (y_pred - y_true))
        changeinw2 = (1/n) * np.dot(np.transpose(affordibility), (y_pred - y_true))
        changeinbi = np.mean(y_pred - y_true)

        #changing as per learning rate 
        w1 = w1 - rate * changeinw1
        w2 = w2 - rate * changeinw2
        bi = bi - rate * changeinbi

        print(f"epochs: {i}, w1: { w1 } , w2: { w2 }, bi: { bi }, cost: {loss}" )

        #base case
        if loss <= loss_threshold:
            break

    return w1, w2, bi

    


gradient_descent(X_train["age"], X_train["affordibility"],1000, y_train, 0.5408)


GD = [0.5806186588831637, 2.5073472557829106, -1.6618241343976377]
model = [0.540692150592804,]


(array([[0.9874324],
        [1.1813993]], dtype=float32),
 array([-0.7780116], dtype=float32))


def gradient_descent(age, affordability, y_true, epochs, loss_thresold):
    w1 = w2 = 1
    bias = 0
    rate = 0.5
    n = len(age)
    for i in range(epochs):
        weighted_sum = w1 * age + w2 * affordability + bias
        y_predicted = sigmoid_numpy(weighted_sum)
        loss = log_loss(y_true, y_predicted)

        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true)) 
        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true)) 

        bias_d = np.mean(y_predicted-y_true)
        w1 = w1 - rate * w1d
        w2 = w2 - rate * w2d
        bias = bias - rate * bias_d

        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')

        if loss<=loss_thresold:
            break

    return w1, w2, bias


gradient_descent(X_train['age'],X_train['affordibility'],y_train,1000, 0.5408)






